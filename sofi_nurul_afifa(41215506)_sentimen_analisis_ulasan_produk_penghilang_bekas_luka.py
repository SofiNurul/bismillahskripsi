# -*- coding: utf-8 -*-
"""SOFI NURUL AFIFA(41215506) - SENTIMEN  ANALISIS ULASAN PRODUK PENGHILANG BEKAS LUKA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p0QcuRQxuctibe5Cy4oT0radF2GiL0P7

# **Import Library**

---
"""

import pandas as pd
import re
import string
import nltk
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from collections import Counter
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_recall_fscore_support

"""Mengunduh resource nltk yang diperlukan


"""

nltk.download('punkt')
nltk.download('stopwords')

"""# **Information Extraction (IE) & Information Retrieval (IR) : Import Dataset**

---

Muat dataset
"""

# Memuat dataset
df = pd.read_csv('data_reviewscr.csv', encoding='MacRoman')
# Menampilkan 5 data teratas dan 5 data terakhir
df_combined = pd.concat([df.head(5), df.tail(5)], axis=0)
# Menampilkan hasil
df_combined

"""# **Natural Language Processing (NLP)**

---

Pembersihan teks
"""

# Fungsi untuk membersihkan teks
def clean_text(teks):
    if not isinstance(teks, str):
        return ""
    teks = teks.lower()
    teks = re.sub(r'http\S+|www\S+|https\S+', '', teks)  # Menghapus URL
    teks = re.sub(r'[^\x00-\x7F]+', ' ', teks)  # Menghilangkan karakter non-ASCII
    teks = teks.translate(str.maketrans('', '', string.punctuation))  # Menghapus tanda baca
    teks = re.sub(r'\d+', '', teks)  # Menghapus angka
    return teks

# Fungsi untuk tokenisasi
def tokenize(teks):
    return teks.split()

# Fungsi untuk menghapus stopwords
def remove_stopwords(tokens, stopwords_set):
    return [word for word in tokens if word not in stopwords_set]

# Fungsi untuk stemming
def apply_stemming(tokens, stemmer):
    return [stemmer.stem(token) for token in tokens]

# Fungsi untuk menjalankan semua tahapan preprocessing
def preprocess_text(teks, stopwords_set, stemmer):
    cleaned_text = clean_text(teks)               # Membersihkan teks
    tokens = tokenize(cleaned_text)               # Tokenisasi
    filtered_tokens = remove_stopwords(tokens, stopwords_set)  # Menghapus stopwords
    stemmed_tokens = apply_stemming(filtered_tokens, stemmer)  # Melakukan stemming
    return {
        "cleaned_text": cleaned_text,
        "tokens": tokens,
        "filtered_tokens": filtered_tokens,
        "stemmed_tokens": stemmed_tokens
    }

"""Inisialisasi Stopwords dan Stemmer"""

stopwords_tambahan = set([
    'gk', 'yg', 'nya', 'dan', 'dll', 'saja', 'juga', 'untuk', 'dengan', 'atau',
    'tapi', 'karena', 'seperti', 'yang', 'saya', 'ga', 'malah', 'tidak', 'lagi',
    'ini', 'di', 'kok', 'aja', 'gak', 'jadi', 'ya', 'dah', 'bisa', 'ada', 'bgt',
    'banget', 'kak', 'ituu', 'grgr', 'jd', 'pengen', 'dgn', 'udah', 'klo', 'tp',
    'pa', 'udh', 'karin', 'bgtt', 'ig', 'po', 'tau', 'utk', 'karna', 'brangnya',
    'ngk', 'kyk', 'sy', 'tp', 'uda', 'nga', 'dngn', 'kalo', 'gamau', 'becak', 'kya',
    'sma', 'jdi', 'ad', 'lg', 'pa', 'huhu', 'dpt', 'gel', 'b', 'jg', 'rb', 'hehe',
    'pdhl', 'dr', 'ku', 'sih', 'ka', 'kyk', 'sy', 'bli', 'skali', 'dikrim', 'berharap',
    'kirim', 'jdi', 'pkai', 'penjual', 'brangnya', 'becak', 'tpi', 'blum', 'bgin','aku','beli','ya','ini','ga','udah','di',
])
nltk_stopwords = set(stopwords.words('indonesian'))  # Mengambil stopwords dari NLTK
final_stopwords = nltk_stopwords.union(stopwords_tambahan)  # Menggabungkan stopwords NLTK dengan tambahan

# Membuat objek stemmer
!pip install Sastrawi
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
factory = StemmerFactory()
stemmer = factory.create_stemmer()

"""Proses Preprocessing Data"""

df_selected = df[['content']]

# Terapkan preprocessing
processed_data = df_selected['content'].apply(
    lambda x: preprocess_text(x, final_stopwords, stemmer)
)

"""Tambahkan kolom hasil preprocessing ke DataFrame"""

# Menambahkan kolom ke DataFrame
df['cleaned_text'] = processed_data.apply(lambda x: x['cleaned_text'])
df['tokens'] = processed_data.apply(lambda x: x['tokens'])
df['filtered_tokens'] = processed_data.apply(lambda x: x['filtered_tokens'])
df['stemmed_tokens'] = processed_data.apply(lambda x: x['stemmed_tokens'])

# Mengatur tampilan Pandas agar teks ditampilkan penuh
pd.set_option('display.max_colwidth', None)
pd.set_option('display.width', 1000)

# Menampilkan tabel hasil preprocessing (10 baris teratas)
df_subset = df[['content', 'cleaned_text', 'tokens', 'filtered_tokens', 'stemmed_tokens']].head(10)

# Menggunakan Pandas Styler untuk menampilkan tabel
styled_table = df_subset.style.set_properties(**{
    'text-align': 'left'  # Atur teks agar rata kiri
}).set_table_styles([{
    'selector': 'th',
    'props': [('text-align', 'left')]  # Atur header kolom agar rata kiri
}])

# Tampilkan tabel
from IPython.display import display
display(styled_table)

import os

# Simpan hasil preprocessing ke file CSV
output_file = "hasil_preprocessing.csv"
df_selected.to_csv(output_file, index=False)

print(f"Hasil preprocessing telah disimpan ke file: {os.path.abspath(output_file)}")

"""# **CLUSTERING**

---

Tranformasi Data Menjadi Fitur Numerik
"""

import pandas as pd
import matplotlib.pyplot as plt

# Load kamus sentimen positif dan negatif, abaikan baris header jika ada
positive_words = pd.read_csv('/content/positive.tsv', sep='\t', header=0, names=['word', 'weight'])
negative_words = pd.read_csv('/content/negative.tsv', sep='\t', header=0, names=['word', 'weight'])

# Buat kamus (dictionary) dari data yang telah di-clean
positive_dict = dict(zip(positive_words['word'].str.strip(), positive_words['weight']))
negative_dict = dict(zip(negative_words['word'].str.strip(), negative_words['weight']))

# Fungsi untuk menghitung skor polaritas
def calculate_polarity_score(words):
    score = 0
    for word in words:  # Iterasi langsung pada setiap kata dalam daftar
        if word in positive_dict:
            score += positive_dict[word]  # Tambahkan bobot dari kamus positif
        elif word in negative_dict:
            score += negative_dict[word]  # Tambahkan bobot dari kamus negatif
    return score

# Fungsi untuk memberi label sentimen berdasarkan skor polaritas
def label_sentiment(score):
    if score >= 0:  # Skor 0 atau lebih besar dianggap positif
        return 'positive'
    else:  # Skor kurang dari 0 dianggap negatif
        return 'negative'

# Terapkan fungsi pada kolom komentar yang sudah di-preprocessing (stemmed_tokens)
df['polarity_score'] = df['stemmed_tokens'].apply(calculate_polarity_score)
df['sentiment'] = df['polarity_score'].apply(label_sentiment)

# Menampilkan hasil
print("\n=== Hasil Klasifikasi Sentimen (Skor 0 atau Lebih Menjadi Positif) ===")
print(df[['stemmed_tokens', 'polarity_score', 'sentiment']].head(10).to_markdown())

# Visualisasi sebaran sentimen
sentiment_counts = df['sentiment'].value_counts()

plt.figure(figsize=(8, 6))
ax = sentiment_counts.plot(kind='bar', color=['green', 'red'])

# Menambahkan angka di atas batang
for idx, value in enumerate(sentiment_counts):
    plt.text(idx, value + 1, str(value), ha='center', va='bottom', fontsize=12)

plt.title("Sebaran Sentimen Positif dan Negatif (Skor 0 Menjadi Positif)")
plt.xlabel("Sentimen")
plt.ylabel("Jumlah Ulasan")
plt.xticks(rotation=0)
plt.grid(axis='y')
plt.show()

"""# **Categorization : Klasifikasi Algortima Naive Bayes**

---

Transformasi Data
"""

# Target adalah label sentimen hasil klustering
X = df['stemmed_tokens']
y = df['sentiment']

# Bagi data menjadi data latih dan data uji
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Gabungkan token menjadi string
X_train = X_train.apply(lambda tokens: ' '.join(tokens))
X_test = X_test.apply(lambda tokens: ' '.join(tokens))

# Inisialisasi TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=1200)

# Fit dan transformasi data latih
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)

# Transformasi data uji
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Tampilkan 5 ulasan teratas setelah transformasi TF-IDF untuk X_train
tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()
tfidf_df_train = pd.DataFrame(X_train_tfidf.toarray(), columns=tfidf_feature_names)

print("=== 5 Ulasan Teratas Setelah Transformasi TF-IDF ===")
print(tfidf_df_train.head(5))

"""Penerapan + Evaluasi Model"""

# Inisialisasi model Naive Bayes
nb_model = MultinomialNB()

# === GridSearchCV untuk Optimasi Model Naive Bayes ===

# Definisikan parameter yang ingin dicari
param_grid = {
    'alpha': [0.1, 0.5, 1.0, 2.0, 5.0]  # Nilai smoothing untuk MultinomialNB
}

# GridSearchCV untuk mencari parameter terbaik
grid_search = GridSearchCV(estimator=nb_model, param_grid=param_grid, scoring='accuracy', cv=5)
grid_search.fit(X_train_tfidf, y_train)

# Menampilkan parameter terbaik
best_params = grid_search.best_params_
print(f"\n=== Parameter Terbaik: {best_params} ===")

# Menggunakan model terbaik untuk prediksi
best_nb_model = grid_search.best_estimator_
y_pred = best_nb_model.predict(X_test_tfidf)

# Evaluasi model
print("\n=== Classification Report ===")
print(classification_report(y_test, y_pred))

accuracy = accuracy_score(y_test, y_pred)
print(f"\n=== Akurasi Model Setelah Optimasi: {accuracy:.2f} ===")

"""# **Visualization**

---

Confusion Matrix
"""

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Tampilkan Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=best_nb_model.classes_, yticklabels=best_nb_model.classes_)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""WORDCLOUD UNTUK SETIAP SENTIMEN"""

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

# Tambahkan stopwords tambahan jika diperlukan
custom_stopwords = STOPWORDS.union({
    'gk', 'yg', 'nya', 'dan', 'dll', 'saja', 'juga', 'untuk', 'dengan', 'atau',
    'tapi', 'karena', 'seperti', 'yang', 'saya', 'ga', 'malah', 'tidak', 'lagi',
    'ini', 'di', 'kok', 'aja', 'gak', 'jadi', 'ya', 'dah', 'bisa', 'ada', 'bgt',
    'banget', 'kak', 'ituu', 'grgr', 'jd', 'pengen', 'dgn', 'udah', 'klo', 'tp',
    'pa', 'udh', 'karin', 'bgtt', 'ig', 'po', 'tau', 'utk', 'karna', 'brangnya',
    'ngk', 'kyk', 'sy', 'tp', 'uda', 'nga', 'dngn', 'kalo', 'gamau', 'becak', 'kya',
    'sma', 'jdi', 'ad', 'lg', 'pa', 'huhu', 'dpt', 'gel', 'b', 'jg', 'rb', 'hehe',
    'pdhl', 'dr', 'ku', 'sih', 'ka', 'kyk', 'sy', 'bli', 'skali', 'dikrim', 'berharap',
    'kirim', 'jdi', 'pkai', 'penjual', 'brangnya', 'becak', 'tpi', 'blum', 'bgin','aku',
    'beli','ya','ini','ga','udah','di','bekas luka', 'moga', 'bekas', 'luka', 'pakai',
    'keloid', 'ilang', 'cepet', 'kulit', 'produk', 'botol', 'scr', 'ampuh', 'cepat',
})

# Gabungkan teks untuk sentimen positif
positive_text = " ".join(df[df['sentiment'] == 'positive']['stemmed_tokens'].apply(lambda x: " ".join(x)))

# Gabungkan teks untuk sentimen negatif
negative_text = " ".join(df[df['sentiment'] == 'negative']['stemmed_tokens'].apply(lambda x: " ".join(x)))

# Buat WordCloud untuk ulasan positif
positive_wordcloud = WordCloud(
    width=800,
    height=400,
    background_color='white',
    stopwords=custom_stopwords,
    colormap='Blues'  # Skema warna untuk positif
).generate(positive_text)

# Buat WordCloud untuk ulasan negatif
negative_wordcloud = WordCloud(
    width=800,
    height=400,
    background_color='white',
    stopwords=custom_stopwords,
    colormap='Reds'  # Skema warna untuk negatif
).generate(negative_text)

# Plot WordCloud untuk positif
plt.figure(figsize=(10, 6))
plt.imshow(positive_wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("WordCloud Ulasan Positif")
plt.show()

# Plot WordCloud untuk negatif
plt.figure(figsize=(10, 6))
plt.imshow(negative_wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("WordCloud Ulasan Negatif")
plt.show()